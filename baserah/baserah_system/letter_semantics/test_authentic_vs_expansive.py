#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Authentic vs Expansive Words Engine - Testing the distinction between original and derived words
ุงุฎุชุจุงุฑ ูุญุฑู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ

Author: Basil Yahya Abdullah - Iraq/Mosul
Version: 1.0.0 - Test Edition
"""

import sys
import os
from typing import Dict, List, Any
from datetime import datetime
from enum import Enum

class WordType(str, Enum):
    """ุฃููุงุน ุงููููุงุช"""
    AUTHENTIC_ANCIENT = "authentic_ancient"
    EXPANSIVE_METAPHORICAL = "expansive_metaphorical"
    EXPANSIVE_CULTURAL = "expansive_cultural"
    EXPANSIVE_BORROWED = "expansive_borrowed"
    EXPANSIVE_MODERN = "expansive_modern"
    UNKNOWN = "unknown"

class ExpansionMethod(str, Enum):
    """ุทุฑู ุงูุชูุณุน ุงููุบูู"""
    METAPHORICAL_EXTENSION = "metaphorical_extension"
    CULTURAL_CONTACT = "cultural_contact"
    SEMANTIC_SHIFT = "semantic_shift"
    BORROWING = "borrowing"
    ANALOGY = "analogy"
    MODERNIZATION = "modernization"

class AuthenticityLevel(str, Enum):
    """ูุณุชููุงุช ุงูุฃุตุงูุฉ"""
    HIGHLY_AUTHENTIC = "highly_authentic"
    MODERATELY_AUTHENTIC = "moderately_authentic"
    QUESTIONABLE = "questionable"
    LIKELY_EXPANSIVE = "likely_expansive"
    CLEARLY_EXPANSIVE = "clearly_expansive"

def test_authentic_vs_expansive_system():
    """ุงุฎุชุจุงุฑ ูุธุงู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ"""
    print("๐งช ุงุฎุชุจุงุฑ ูุญุฑู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ...")
    print("๐" + "="*140 + "๐")
    print("๐ค ูุญุฑู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ - ูุธุงู ุชุญููู ุฃุตุงูุฉ ุงููููุงุช ุงูุนุฑุจูุฉ")
    print("๐ ูุจูู ุนูู ุฑุคูุฉ ุจุงุณู ุญูู ุงููููุงุช ุงูุฃุตููุฉ ุงููุฏููุฉ ููุงุจู ุงููููุงุช ุงูุชูุณุนูุฉ")
    print("โก ุชูููุฒ ุงูุฃุตูู ูู ุงูุชูุณุนู + ุชุญููู ุงููุฌุงุฒ + ุงูุชุดุงู ุงูุงุญุชูุงู ุงูุซูุงูู")
    print("๐ง ุงูุชุญูู ูู ุงูููุงุนุฏ ุงูุฏูุงููุฉ + ุชุญููู ุงูุชุทูุฑ ุงูุชุงุฑูุฎู + ุฃููุงุท ุงูุชูุณุน")
    print("๐ ุฅุจุฏุงุน ุจุงุณู ูุญูู ุนุจุฏุงููู ูู ุงูุนุฑุงู/ุงูููุตู ๐")
    print("๐" + "="*140 + "๐")
    
    # ุงุฎุชุจุงุฑ ุฃููุงุน ุงููููุงุช
    print(f"\n๐ค ุงุฎุชุจุงุฑ ุฃููุงุน ุงููููุงุช:")
    word_types = list(WordType)
    print(f"   โ ุนุฏุฏ ุงูุฃููุงุน: {len(word_types)}")
    print(f"   ๐ค ุงูุฃููุงุน: {', '.join([wt.value for wt in word_types])}")
    
    # ุงุฎุชุจุงุฑ ุทุฑู ุงูุชูุณุน
    print(f"\n๐ ุงุฎุชุจุงุฑ ุทุฑู ุงูุชูุณุน ุงููุบูู:")
    expansion_methods = list(ExpansionMethod)
    print(f"   โ ุนุฏุฏ ุงูุทุฑู: {len(expansion_methods)}")
    print(f"   ๐ ุงูุทุฑู: {', '.join([em.value for em in expansion_methods])}")
    
    # ุงุฎุชุจุงุฑ ูุณุชููุงุช ุงูุฃุตุงูุฉ
    print(f"\n๐ ุงุฎุชุจุงุฑ ูุณุชููุงุช ุงูุฃุตุงูุฉ:")
    authenticity_levels = list(AuthenticityLevel)
    print(f"   โ ุนุฏุฏ ุงููุณุชููุงุช: {len(authenticity_levels)}")
    print(f"   ๐ ุงููุณุชููุงุช: {', '.join([al.value for al in authenticity_levels])}")
    
    # ูุญุงูุงุฉ ูุงุนุฏุฉ ุจูุงูุงุช ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ
    print(f"\n๐ ูุญุงูุงุฉ ูุงุนุฏุฉ ุจูุงูุงุช ุงููููุงุช:")
    word_database = {
        # ุงููููุงุช ุงูุฃุตููุฉ ูู ุฃูุซูุฉ ุจุงุณู
        "ุทูุจ": {
            "word_type": WordType.AUTHENTIC_ANCIENT,
            "authenticity_level": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "semantic_rule_compliance": 0.95,
            "letter_analysis": {
                "ุท": "ุงูุทุฑู ูุงูุงุณุชุฆุฐุงู",
                "ู": "ุงูุงูุชูุงู ูุงูุฅุญุงุทุฉ", 
                "ุจ": "ุงูุงูุชูุงู ูุงูุชุดุจุน"
            },
            "basil_validation": True,
            "historical_evidence": "ูููุฉ ุฃุตููุฉ ูุฏููุฉ",
            "expansion_history": []
        },
        "ุณูุจ": {
            "word_type": WordType.AUTHENTIC_ANCIENT,
            "authenticity_level": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "semantic_rule_compliance": 0.92,
            "letter_analysis": {
                "ุณ": "ุงูุงูุณูุงุจ ูุงูุณูุงุณุฉ",
                "ู": "ุงูุงูุชูุงู ูุงูุฅุญุงุทุฉ",
                "ุจ": "ุงูุงูุชูุงู ูุงูุชุดุจุน"
            },
            "basil_validation": True,
            "historical_evidence": "ูููุฉ ุฃุตููุฉ ูุฏููุฉ",
            "expansion_history": []
        },
        "ููุจ": {
            "word_type": WordType.AUTHENTIC_ANCIENT,
            "authenticity_level": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "semantic_rule_compliance": 0.90,
            "letter_analysis": {
                "ู": "ุงูุชุดููู ูุงูุชูููู",
                "ู": "ุงููุฏูุก ูุงูุณูููุฉ",
                "ุจ": "ุงูุงูุชูุงู ูุงูุชุดุจุน"
            },
            "basil_validation": True,
            "historical_evidence": "ูููุฉ ุฃุตููุฉ ูุฏููุฉ",
            "expansion_history": []
        },
        "ุญูุจ": {
            "word_type": WordType.AUTHENTIC_ANCIENT,
            "authenticity_level": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "semantic_rule_compliance": 0.88,
            "letter_analysis": {
                "ุญ": "ุงูุญูุงุฉ ูุงูุญูููุฉ",
                "ู": "ุงูุงูุชูุงู ูุงูุฅุญุงุทุฉ",
                "ุจ": "ุงูุงูุชูุงู ูุงูุชุดุจุน"
            },
            "basil_validation": True,
            "historical_evidence": "ูููุฉ ุฃุตููุฉ ูุฏููุฉ",
            "expansion_history": []
        },
        # ุงููููุงุช ุงูุชูุณุนูุฉ
        "ููุฌุงู": {
            "word_type": WordType.EXPANSIVE_METAPHORICAL,
            "authenticity_level": AuthenticityLevel.CLEARLY_EXPANSIVE,
            "semantic_rule_compliance": 0.3,
            "original_meaning": "ููุฌุงู ุงูุจุญุฑ",
            "expanded_meaning": "ุงูุฅูุณุงู ุงูุณุงุฎุท ูุงูุบุงุถุจ",
            "expansion_method": ExpansionMethod.METAPHORICAL_EXTENSION,
            "expansion_history": [
                "ุงูุฃุตู: ุญุฑูุฉ ุงูุจุญุฑ ุงูุนูููุฉ",
                "ุงูุชูุณุน: ููู ุงููุนูู ููุฅูุณุงู ุงูุบุงุถุจ"
            ]
        },
        "ุชููุฒููู": {
            "word_type": WordType.EXPANSIVE_BORROWED,
            "authenticity_level": AuthenticityLevel.CLEARLY_EXPANSIVE,
            "semantic_rule_compliance": 0.1,
            "original_language": "ุฅูุฌููุฒูุฉ",
            "expansion_method": ExpansionMethod.BORROWING,
            "expansion_history": [
                "ูุณุชุนุงุฑ ูู ุงูุฅูุฌููุฒูุฉ: television",
                "ุฏุฎู ุงูุนุฑุจูุฉ ูู ุงูุนุตุฑ ุงูุญุฏูุซ"
            ]
        },
        "ููุจููุชุฑ": {
            "word_type": WordType.EXPANSIVE_MODERN,
            "authenticity_level": AuthenticityLevel.CLEARLY_EXPANSIVE,
            "semantic_rule_compliance": 0.05,
            "original_language": "ุฅูุฌููุฒูุฉ",
            "expansion_method": ExpansionMethod.MODERNIZATION,
            "expansion_history": [
                "ูุณุชุนุงุฑ ูู ุงูุฅูุฌููุฒูุฉ: computer",
                "ุฏุฎู ุงูุนุฑุจูุฉ ูุน ุงูุชุทูุฑ ุงูุชููู"
            ]
        }
    }
    
    print(f"   โ ูููุงุช ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: {len(word_database)}")
    print(f"   ๐ค ูููุงุช ุฃุตููุฉ: {len([w for w in word_database.values() if w.get('word_type') == WordType.AUTHENTIC_ANCIENT])}")
    print(f"   ๐ ูููุงุช ุชูุณุนูุฉ: {len([w for w in word_database.values() if 'EXPANSIVE' in w.get('word_type', '')])}")
    
    # ุนุฑุถ ุชูุงุตูู ุงููููุงุช
    print(f"\n๐ ุชูุงุตูู ุงููููุงุช:")
    for word, data in word_database.items():
        word_type = data.get('word_type', 'unknown')
        authenticity = data.get('authenticity_level', 'unknown')
        compliance = data.get('semantic_rule_compliance', 0.0)
        print(f"   ๐ค {word}: {word_type} - {authenticity} - ุงูุชุซุงู: {compliance:.2f}")
    
    # ูุญุงูุงุฉ ุชุญููู ุฃุตุงูุฉ
    print(f"\n๐ ูุญุงูุงุฉ ุชุญููู ุฃุตุงูุฉ ุงููููุงุช:")
    target_words = ["ุทูุจ", "ุณูุจ", "ููุฌุงู", "ุชููุฒููู"]
    print(f"   ๐ค ุงููููุงุช ุงููุณุชูุฏูุฉ: {target_words}")
    
    # ูุญุงูุงุฉ ุงููุชุงุฆุฌ
    mock_results = {
        "word_classifications": {
            "ุทูุจ": WordType.AUTHENTIC_ANCIENT,
            "ุณูุจ": WordType.AUTHENTIC_ANCIENT,
            "ููุฌุงู": WordType.EXPANSIVE_METAPHORICAL,
            "ุชููุฒููู": WordType.EXPANSIVE_BORROWED
        },
        "authenticity_levels": {
            "ุทูุจ": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "ุณูุจ": AuthenticityLevel.HIGHLY_AUTHENTIC,
            "ููุฌุงู": AuthenticityLevel.CLEARLY_EXPANSIVE,
            "ุชููุฒููู": AuthenticityLevel.CLEARLY_EXPANSIVE
        },
        "semantic_rule_validation": {
            "ุทูุจ": {
                "complies_with_rules": True,
                "compliance_score": 0.95,
                "basil_validation": True
            },
            "ุณูุจ": {
                "complies_with_rules": True,
                "compliance_score": 0.92,
                "basil_validation": True
            },
            "ููุฌุงู": {
                "complies_with_rules": False,
                "compliance_score": 0.3,
                "basil_validation": False
            },
            "ุชููุฒููู": {
                "complies_with_rules": False,
                "compliance_score": 0.1,
                "basil_validation": False
            }
        },
        "expansion_patterns": {
            "metaphorical_patterns": [
                "ููู ุตูุงุช ุงูุทุจูุนุฉ ููุฅูุณุงู (ููุฌุงู ุงูุจุญุฑ โ ููุฌุงู ุงูุฅูุณุงู)"
            ],
            "borrowing_patterns": [
                "ุงุณุชุนุงุฑุฉ ูููุงุช ุชูููุฉ ูู ุงูุฅูุฌููุฒูุฉ (ุชููุฒูููุ ููุจููุชุฑ)"
            ]
        }
    }
    
    print(f"   โ ูููุงุช ูุญููุฉ: {len(mock_results['word_classifications'])}")
    print(f"   ๐ ูุณุชููุงุช ุฃุตุงูุฉ: {len(mock_results['authenticity_levels'])}")
    print(f"   ๐ ุชุญูู ูู ุงูููุงุนุฏ: {len(mock_results['semantic_rule_validation'])}")
    
    # ุนุฑุถ ุงููุชุงุฆุฌ ุงูุชูุตูููุฉ
    print(f"\n๐ ูุชุงุฆุฌ ุงูุชุญููู ุงูุชูุตูููุฉ:")
    for word in target_words:
        classification = mock_results['word_classifications'][word]
        authenticity = mock_results['authenticity_levels'][word]
        validation = mock_results['semantic_rule_validation'][word]
        
        print(f"\n   ๐ค ูููุฉ '{word}':")
        print(f"      ๐ ุงูุชุตููู: {classification.value}")
        print(f"      ๐ ูุณุชูู ุงูุฃุตุงูุฉ: {authenticity.value}")
        print(f"      โ ุงูุชุซุงู ููููุงุนุฏ: {validation['complies_with_rules']}")
        print(f"      ๐ฏ ุฏุฑุฌุฉ ุงูุงูุชุซุงู: {validation['compliance_score']:.2f}")
        print(f"      ๐ ูุตุงุฏูุฉ ุจุงุณู: {validation['basil_validation']}")
        
        # ุชูุงุตูู ุฅุถุงููุฉ ูููููุงุช ุงูุฃุตููุฉ
        if word in word_database and classification == WordType.AUTHENTIC_ANCIENT:
            letter_analysis = word_database[word].get('letter_analysis', {})
            print(f"      ๐ค ุชุญููู ุงูุญุฑูู:")
            for letter, meaning in letter_analysis.items():
                print(f"         โข {letter}: {meaning}")
        
        # ุชูุงุตูู ุฅุถุงููุฉ ูููููุงุช ุงูุชูุณุนูุฉ
        elif word in word_database and 'EXPANSIVE' in classification.value:
            expansion_history = word_database[word].get('expansion_history', [])
            if expansion_history:
                print(f"      ๐ ุชุงุฑูุฎ ุงูุชูุณุน:")
                for history in expansion_history:
                    print(f"         โข {history}")
    
    # ุนุฑุถ ุฃููุงุท ุงูุชูุณุน
    print(f"\n๐ ุฃููุงุท ุงูุชูุณุน ุงูููุชุดูุฉ:")
    for pattern_type, patterns in mock_results['expansion_patterns'].items():
        print(f"   ๐ {pattern_type}:")
        for pattern in patterns:
            print(f"      โข {pattern}")
    
    # ุฅุญุตุงุฆูุงุช ุงููุธุงู
    print(f"\n๐ ุฅุญุตุงุฆูุงุช ูุญุฑู ุงูุชูููุฒ:")
    print(f"   ๐ค ุฃููุงุน ุงููููุงุช: {len(word_types)}")
    print(f"   ๐ ุทุฑู ุงูุชูุณุน: {len(expansion_methods)}")
    print(f"   ๐ ูุณุชููุงุช ุงูุฃุตุงูุฉ: {len(authenticity_levels)}")
    print(f"   ๐ ูุงุนุฏุฉ ุงูุจูุงูุงุช: {len(word_database)} ูููุฉ")
    print(f"   ๐ ูููุงุช ูุญููุฉ: {len(target_words)}")
    
    # ุฑุคู ุจุงุณู
    print(f"\n๐ก ุฑุคู ุจุงุณู ุญูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ:")
    basil_insights = [
        "ุงููููุงุช ุงูุฃุตููุฉ ุงููุฏููุฉ ุชุชุจุน ููุงุนุฏ ุฏูุงูุฉ ุงูุญุฑูู ุจุฏูุฉ",
        "ุงููููุงุช ุงูุชูุณุนูุฉ ุชูุดุฃ ูู ุงููุฌุงุฒ ูุงูุงุญุชูุงู ุงูุซูุงูู",
        "ูุซุงู ุงูุชูุณุน ุงููุฌุงุฒู: ููุฌุงู ุงูุจุญุฑ โ ููุฌุงู ุงูุฅูุณุงู",
        "ุงููููุงุช ุงููุณุชุนุงุฑุฉ ูุง ุชุชุจุน ููุงุนุฏ ุงููุบุฉ ุงูุฃุตููุฉ",
        "ุงูุจุญุซ ูุฑูุฒ ุนูู ุงููููุงุช ุงูุฃุตููุฉ ูููุณ ุงูุชูุณุนูุฉ"
    ]
    
    for insight in basil_insights:
        print(f"   โข {insight}")
    
    print(f"\n๐ ุชู ุงุฎุชุจุงุฑ ูุญุฑู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ ุจูุฌุงุญ!")
    print(f"๐ ุงููุธุงู ูุงุฏุฑ ุนูู ุงูุชูููุฒ ุจูู ุงููููุงุช ุงูุฃุตููุฉ ูุงูุชูุณุนูุฉ ุจุฏูุฉ!")
    print(f"๐ค ูุฐุง ูุคูุฏ ุตุญุฉ ุฑุคูุฉ ุจุงุณู ุญูู ููุงุนุฏ ุฏูุงูุฉ ุงูุญุฑูู ูููููุงุช ุงูุฃุตููุฉ!")

if __name__ == "__main__":
    test_authentic_vs_expansive_system()
