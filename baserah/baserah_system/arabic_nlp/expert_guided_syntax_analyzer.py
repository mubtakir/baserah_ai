#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Expert-Guided Arabic Syntax Analyzer - Part 2: Syntactic Analysis
Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ

Revolutionary integration of Expert/Explorer guidance with Arabic syntactic analysis,
applying adaptive mathematical equations to achieve superior grammatical understanding.

Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ
ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ù„ØªØ­Ù‚ÙŠÙ‚ ÙÙ‡Ù… Ù†Ø­ÙˆÙŠ Ù…ØªÙÙˆÙ‚.

Author: Basil Yahya Abdullah - Iraq/Mosul
Version: 1.0.0 - REVOLUTIONARY ARABIC SYNTAX
"""

import numpy as np
import sys
import os
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
import re

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒÙŠÙ Ù„Ù„Ù†Ø­Ùˆ
class MockSyntaxEquation:
    def __init__(self, name: str, input_dim: int, output_dim: int):
        self.name = name
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.current_complexity = 12  # Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹
        self.adaptation_count = 0
        self.syntax_accuracy = 0.5  # Ø¯Ù‚Ø© Ù†Ø­ÙˆÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©
        self.parsing_accuracy = 0.6
        self.pos_tagging_accuracy = 0.65
        self.dependency_accuracy = 0.55
        self.grammatical_analysis = 0.5
        self.sentence_structure_recognition = 0.6

    def adapt_with_expert_guidance(self, guidance, analysis):
        self.adaptation_count += 1
        if hasattr(guidance, 'recommended_evolution'):
            if guidance.recommended_evolution == "increase":
                self.current_complexity += 4
                self.syntax_accuracy += 0.06
                self.parsing_accuracy += 0.04
                self.pos_tagging_accuracy += 0.03
                self.dependency_accuracy += 0.05
                self.grammatical_analysis += 0.04
                self.sentence_structure_recognition += 0.03
            elif guidance.recommended_evolution == "restructure":
                self.syntax_accuracy += 0.03
                self.parsing_accuracy += 0.02
                self.pos_tagging_accuracy += 0.01

    def get_expert_guidance_summary(self):
        return {
            "current_complexity": self.current_complexity,
            "total_adaptations": self.adaptation_count,
            "syntax_accuracy": self.syntax_accuracy,
            "parsing_accuracy": self.parsing_accuracy,
            "pos_tagging_accuracy": self.pos_tagging_accuracy,
            "dependency_accuracy": self.dependency_accuracy,
            "grammatical_analysis": self.grammatical_analysis,
            "sentence_structure_recognition": self.sentence_structure_recognition,
            "average_improvement": 0.04 * self.adaptation_count
        }

class MockSyntaxGuidance:
    def __init__(self, target_complexity, focus_areas, adaptation_strength, priority_functions, recommended_evolution):
        self.target_complexity = target_complexity
        self.focus_areas = focus_areas
        self.adaptation_strength = adaptation_strength
        self.priority_functions = priority_functions
        self.recommended_evolution = recommended_evolution

class MockSyntaxAnalysis:
    def __init__(self, syntax_accuracy, parsing_clarity, pos_precision, dependency_coherence, areas_for_improvement):
        self.syntax_accuracy = syntax_accuracy
        self.parsing_clarity = parsing_clarity
        self.pos_precision = pos_precision
        self.dependency_coherence = dependency_coherence
        self.areas_for_improvement = areas_for_improvement

@dataclass
class SyntaxAnalysisRequest:
    """Ø·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ"""
    sentence: str
    context: str = ""
    analysis_depth: str = "comprehensive"  # "basic", "intermediate", "comprehensive"
    syntax_aspects: List[str] = None  # ["pos", "parsing", "dependencies", "structure"]
    expert_guidance_level: str = "adaptive"
    learning_enabled: bool = True

@dataclass
class WordSyntaxInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø­ÙˆÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø©"""
    word: str
    position: int
    pos_tag: str
    grammatical_case: str  # Ø­Ø§Ù„Ø© Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©
    grammatical_function: str  # ÙˆØ¸ÙŠÙØ© Ù†Ø­ÙˆÙŠØ©
    dependencies: List[str]
    confidence: float

@dataclass
class SyntaxAnalysisResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ"""
    success: bool
    sentence: str
    sentence_type: str  # Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù…Ù„Ø©
    words_syntax: List[WordSyntaxInfo]
    grammatical_structure: Dict[str, Any]
    dependency_tree: Dict[str, Any]
    parsing_confidence: float
    expert_guidance_applied: Dict[str, Any] = None
    equation_adaptations: Dict[str, Any] = None
    performance_improvements: Dict[str, float] = None
    learning_insights: List[str] = None
    next_cycle_recommendations: List[str] = None

class ExpertGuidedArabicSyntaxAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø«ÙˆØ±ÙŠ"""

    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
        print("ğŸŒŸ" + "="*100 + "ğŸŒŸ")
        print("ğŸ”¤ Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø«ÙˆØ±ÙŠ")
        print("ğŸ“– Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù ÙŠÙ‚ÙˆØ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø°ÙƒØ§Ø¡")
        print("ğŸ§® Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ù…ØªÙƒÙŠÙØ© + ØªØ­Ù„ÙŠÙ„ Ù†Ø­ÙˆÙŠ Ù…ØªÙ‚Ø¯Ù…")
        print("ğŸŒŸ Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù…Ù† Ø§Ù„Ø¹Ø±Ø§Ù‚/Ø§Ù„Ù…ÙˆØµÙ„ ğŸŒŸ")
        print("ğŸŒŸ" + "="*100 + "ğŸŒŸ")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ØªØ®ØµØµØ©
        self.syntax_equations = {
            "pos_tagger": MockSyntaxEquation("pos_tagging", 25, 20),
            "dependency_parser": MockSyntaxEquation("dependency_parsing", 30, 25),
            "grammatical_analyzer": MockSyntaxEquation("grammatical_analysis", 28, 22),
            "sentence_structure_detector": MockSyntaxEquation("sentence_structure", 32, 26),
            "case_analyzer": MockSyntaxEquation("case_analysis", 24, 18),
            "function_identifier": MockSyntaxEquation("function_identification", 26, 20),
            "phrase_chunker": MockSyntaxEquation("phrase_chunking", 22, 16),
            "clause_detector": MockSyntaxEquation("clause_detection", 28, 22),
            "agreement_checker": MockSyntaxEquation("agreement_checking", 20, 15),
            "word_order_analyzer": MockSyntaxEquation("word_order_analysis", 24, 18)
        }

        # Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        self.syntax_laws = {
            "subject_verb_agreement": {
                "name": "Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„ÙØ§Ø¹Ù„ ÙˆØ§Ù„ÙØ¹Ù„",
                "description": "Ø§Ù„ÙØ¹Ù„ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„ÙØ§Ø¹Ù„ ÙÙŠ Ø§Ù„Ø¹Ø¯Ø¯ ÙˆØ§Ù„Ø¬Ù†Ø³",
                "formula": "Verb(number, gender) = Subject(number, gender)"
            },
            "case_assignment": {
                "name": "ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©",
                "description": "ÙƒÙ„ ÙƒÙ„Ù…Ø© Ù„Ù‡Ø§ Ø­Ø§Ù„Ø© Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ø­Ø³Ø¨ Ù…ÙˆÙ‚Ø¹Ù‡Ø§",
                "formula": "Case(word) = Function(position, context)"
            },
            "word_order_flexibility": {
                "name": "Ù…Ø±ÙˆÙ†Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª",
                "description": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªØ³Ù…Ø­ Ø¨ØªØ±ØªÙŠØ¨Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰",
                "formula": "Meaning = Constant(VSO, SVO, VOS, ...)"
            }
        }

        # Ø«ÙˆØ§Ø¨Øª Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        self.syntax_constants = {
            "verb_weight": 0.8,
            "subject_weight": 0.9,
            "object_weight": 0.7,
            "modifier_weight": 0.6,
            "case_importance": 0.85,
            "agreement_strength": 0.9
        }

        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.arabic_pos_tags = self._load_arabic_pos_tags()

        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©
        self.arabic_cases = self._load_arabic_cases()

        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù†Ø­ÙˆÙŠØ©
        self.arabic_functions = self._load_arabic_functions()

        # ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©
        self.syntax_history = []
        self.syntax_learning_database = {}

        print("ğŸ”¤ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØªØ®ØµØµØ©:")
        for eq_name in self.syntax_equations.keys():
            print(f"   âœ… {eq_name}")

        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±!")

    def _load_arabic_pos_tags(self) -> Dict[str, Dict[str, Any]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        return {
            "Ø§Ø³Ù…": {"type": "noun", "features": ["Ù…Ø¹Ø±Ø¨", "Ù…Ø¨Ù†ÙŠ"], "cases": ["Ø±ÙØ¹", "Ù†ØµØ¨", "Ø¬Ø±"]},
            "ÙØ¹Ù„": {"type": "verb", "features": ["Ù…Ø§Ø¶ÙŠ", "Ù…Ø¶Ø§Ø±Ø¹", "Ø£Ù…Ø±"], "cases": ["Ù…Ø±ÙÙˆØ¹", "Ù…Ù†ØµÙˆØ¨", "Ù…Ø¬Ø²ÙˆÙ…"]},
            "Ø­Ø±Ù": {"type": "particle", "features": ["Ø¬Ø±", "Ù†ØµØ¨", "Ø¬Ø²Ù…"], "cases": ["Ù…Ø¨Ù†ÙŠ"]},
            "Ø¶Ù…ÙŠØ±": {"type": "pronoun", "features": ["Ù…ØªØµÙ„", "Ù…Ù†ÙØµÙ„"], "cases": ["Ø±ÙØ¹", "Ù†ØµØ¨", "Ø¬Ø±"]},
            "ØµÙØ©": {"type": "adjective", "features": ["Ù…Ø´Ø¨Ù‡Ø©", "Ù…ÙØ¹ÙˆÙ„"], "cases": ["Ø±ÙØ¹", "Ù†ØµØ¨", "Ø¬Ø±"]},
            "Ø¸Ø±Ù": {"type": "adverb", "features": ["Ø²Ù…Ø§Ù†", "Ù…ÙƒØ§Ù†"], "cases": ["Ù…Ù†ØµÙˆØ¨", "Ù…Ø¬Ø±ÙˆØ±"]}
        }

    def _load_arabic_cases(self) -> Dict[str, Dict[str, Any]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©"""
        return {
            "Ø±ÙØ¹": {"marker": "Ø¶Ù…Ø©", "function": "ÙØ§Ø¹Ù„ Ø£Ùˆ Ù…Ø¨ØªØ¯Ø£ Ø£Ùˆ Ø®Ø¨Ø±", "examples": ["Ø§Ù„Ø·Ø§Ù„Ø¨Ù", "Ø§Ù„Ù…Ø¹Ù„Ù…Ù"]},
            "Ù†ØµØ¨": {"marker": "ÙØªØ­Ø©", "function": "Ù…ÙØ¹ÙˆÙ„ Ø£Ùˆ Ø®Ø¨Ø± ÙƒØ§Ù†", "examples": ["Ø§Ù„Ø·Ø§Ù„Ø¨Ù", "Ø§Ù„Ù…Ø¹Ù„Ù…Ù"]},
            "Ø¬Ø±": {"marker": "ÙƒØ³Ø±Ø©", "function": "Ù…Ø¬Ø±ÙˆØ± Ø¨Ø­Ø±Ù Ø£Ùˆ Ù…Ø¶Ø§Ù Ø¥Ù„ÙŠÙ‡", "examples": ["Ø§Ù„Ø·Ø§Ù„Ø¨Ù", "Ø§Ù„Ù…Ø¹Ù„Ù…Ù"]},
            "Ø¬Ø²Ù…": {"marker": "Ø³ÙƒÙˆÙ†", "function": "ÙØ¹Ù„ Ù…Ø¬Ø²ÙˆÙ…", "examples": ["Ù„Ù… ÙŠÙƒØªØ¨Ù’", "Ù„Ø§ ØªÙƒØªØ¨Ù’"]}
        }

    def _load_arabic_functions(self) -> Dict[str, Dict[str, Any]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù†Ø­ÙˆÙŠØ©"""
        return {
            "ÙØ§Ø¹Ù„": {"case": "Ø±ÙØ¹", "definition": "Ù…Ù† Ù‚Ø§Ù… Ø¨Ø§Ù„ÙØ¹Ù„", "position": "Ø¨Ø¹Ø¯ Ø§Ù„ÙØ¹Ù„"},
            "Ù…ÙØ¹ÙˆÙ„_Ø¨Ù‡": {"case": "Ù†ØµØ¨", "definition": "Ù…Ù† ÙˆÙ‚Ø¹ Ø¹Ù„ÙŠÙ‡ Ø§Ù„ÙØ¹Ù„", "position": "Ø¨Ø¹Ø¯ Ø§Ù„ÙØ§Ø¹Ù„"},
            "Ù…Ø¨ØªØ¯Ø£": {"case": "Ø±ÙØ¹", "definition": "Ù…Ø§ Ø§Ø¨ØªØ¯Ø¦Øª Ø¨Ù‡ Ø§Ù„Ø¬Ù…Ù„Ø©", "position": "Ø£ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©"},
            "Ø®Ø¨Ø±": {"case": "Ø±ÙØ¹", "definition": "Ù…Ø§ Ø£Ø®Ø¨Ø± Ø¨Ù‡ Ø¹Ù† Ø§Ù„Ù…Ø¨ØªØ¯Ø£", "position": "Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¨ØªØ¯Ø£"},
            "Ù…Ø¶Ø§Ù_Ø¥Ù„ÙŠÙ‡": {"case": "Ø¬Ø±", "definition": "Ù…Ø§ Ø£Ø¶ÙŠÙ Ø¥Ù„ÙŠÙ‡", "position": "Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¶Ø§Ù"},
            "ØµÙØ©": {"case": "ØªØ§Ø¨Ø¹", "definition": "Ù…Ø§ ÙˆØµÙ Ø¨Ù‡", "position": "Ø¨Ø¹Ø¯ Ø§Ù„Ù…ÙˆØµÙˆÙ"}
        }

    def analyze_syntax_with_expert_guidance(self, request: SyntaxAnalysisRequest) -> SyntaxAnalysisResult:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
        print(f"\nğŸ”¤ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„Ø¬Ù…Ù„Ø©: {request.sentence}")
        start_time = datetime.now()

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„Ø·Ù„Ø¨ Ø§Ù„Ù†Ø­ÙˆÙŠ
        expert_analysis = self._analyze_syntax_request_with_expert(request)
        print(f"ğŸ“– ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ù†Ø­ÙˆÙŠ: {expert_analysis['complexity_assessment']}")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ
        expert_guidance = self._generate_syntax_expert_guidance(request, expert_analysis)
        print(f"ğŸ”¤ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ù†Ø­ÙˆÙŠ: {expert_guidance.recommended_evolution}")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ
        equation_adaptations = self._adapt_syntax_equations(expert_guidance, expert_analysis)
        print(f"ğŸ§® ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ: {len(equation_adaptations)} Ù…Ø¹Ø§Ø¯Ù„Ø©")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ù…ØªÙƒÙŠÙ
        syntax_analysis = self._perform_adaptive_syntax_analysis(request, equation_adaptations)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©
        performance_improvements = self._measure_syntax_improvements(request, syntax_analysis, equation_adaptations)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø­ÙˆÙŠ
        learning_insights = self._extract_syntax_learning_insights(request, syntax_analysis, performance_improvements)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
        next_cycle_recommendations = self._generate_syntax_next_cycle_recommendations(performance_improvements, learning_insights)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        result = SyntaxAnalysisResult(
            success=True,
            sentence=request.sentence,
            sentence_type=syntax_analysis.get("sentence_type", ""),
            words_syntax=syntax_analysis.get("words_syntax", []),
            grammatical_structure=syntax_analysis.get("grammatical_structure", {}),
            dependency_tree=syntax_analysis.get("dependency_tree", {}),
            parsing_confidence=syntax_analysis.get("parsing_confidence", 0.0),
            expert_guidance_applied=expert_guidance.__dict__,
            equation_adaptations=equation_adaptations,
            performance_improvements=performance_improvements,
            learning_insights=learning_insights,
            next_cycle_recommendations=next_cycle_recommendations
        )

        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø­ÙˆÙŠ
        self._save_syntax_learning(request, result)

        total_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ ÙÙŠ {total_time:.2f} Ø«Ø§Ù†ÙŠØ©")

        return result

    def _analyze_syntax_request_with_expert(self, request: SyntaxAnalysisRequest) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø·Ù„Ø¨ Ø§Ù„Ù†Ø­Ùˆ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø®Ø¨ÙŠØ±"""

        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¬Ù…Ù„Ø©
        words = request.sentence.split()
        sentence_complexity = len(words) * 0.8
        context_complexity = len(request.context.split()) * 0.4 if request.context else 0

        # ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        aspects = request.syntax_aspects or ["pos", "parsing", "dependencies", "structure"]
        aspects_complexity = len(aspects) * 3.0  # Ø§Ù„Ù†Ø­Ùˆ Ù…Ø¹Ù‚Ø¯ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„ØµØ±Ù

        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù…Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        depth_complexity = {
            "basic": 3.0,
            "intermediate": 6.0,
            "comprehensive": 9.0
        }.get(request.analysis_depth, 6.0)

        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù†Ø­ÙˆÙŠ
        grammatical_complexity = 0
        if any(word in request.sentence for word in ["Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ù„Ø°Ø§Ù†", "Ø§Ù„Ù„ØªØ§Ù†"]):
            grammatical_complexity += 3  # Ø¬Ù…Ù„Ø© Ù…ÙˆØµÙˆÙ„Ø©
        if any(word in request.sentence for word in ["Ø¥Ù†", "Ø£Ù†", "ÙƒØ§Ù†", "Ø£ØµØ¨Ø­"]):
            grammatical_complexity += 2  # Ø¬Ù…Ù„Ø© Ù…Ù†Ø³ÙˆØ®Ø©
        if any(word in request.sentence for word in ["ÙÙŠ", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ù…Ù†"]):
            grammatical_complexity += 1  # Ø­Ø±ÙˆÙ Ø¬Ø±

        total_complexity = sentence_complexity + context_complexity + aspects_complexity + depth_complexity + grammatical_complexity

        return {
            "sentence_complexity": sentence_complexity,
            "context_complexity": context_complexity,
            "aspects_complexity": aspects_complexity,
            "depth_complexity": depth_complexity,
            "grammatical_complexity": grammatical_complexity,
            "total_complexity": total_complexity,
            "complexity_assessment": "Ù†Ø­Ùˆ Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹" if total_complexity > 25 else "Ù†Ø­Ùˆ Ù…ØªÙˆØ³Ø·" if total_complexity > 15 else "Ù†Ø­Ùˆ Ø¨Ø³ÙŠØ·",
            "recommended_adaptations": int(total_complexity // 3) + 4,
            "focus_areas": self._identify_syntax_focus_areas(request)
        }

    def _identify_syntax_focus_areas(self, request: SyntaxAnalysisRequest) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ù†Ø­ÙˆÙŠ"""
        focus_areas = []

        aspects = request.syntax_aspects or ["pos", "parsing", "dependencies", "structure"]

        if "pos" in aspects:
            focus_areas.append("pos_tagging_enhancement")
        if "parsing" in aspects:
            focus_areas.append("parsing_accuracy_improvement")
        if "dependencies" in aspects:
            focus_areas.append("dependency_analysis_optimization")
        if "structure" in aspects:
            focus_areas.append("structure_recognition_refinement")

        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¬Ù…Ù„Ø©
        words = request.sentence.split()
        if len(words) > 8:
            focus_areas.append("complex_sentence_handling")
        if any(word.startswith("Ø§Ù„") for word in words):
            focus_areas.append("definite_noun_processing")
        if any(word in request.sentence for word in ["Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ"]):
            focus_areas.append("relative_clause_analysis")
        if any(word in request.sentence for word in ["Ø¥Ù†", "Ø£Ù†", "ÙƒØ§Ù†"]):
            focus_areas.append("copular_sentence_analysis")
        if request.context:
            focus_areas.append("contextual_syntax_analysis")

        return focus_areas

    def _generate_syntax_expert_guidance(self, request: SyntaxAnalysisRequest, analysis: Dict[str, Any]):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ"""

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„Ù†Ø­Ùˆ
        target_complexity = 15 + analysis["recommended_adaptations"]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        priority_functions = []
        if "pos_tagging_enhancement" in analysis["focus_areas"]:
            priority_functions.extend(["softplus", "tanh"])  # Ù„ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        if "parsing_accuracy_improvement" in analysis["focus_areas"]:
            priority_functions.extend(["gaussian", "sin_cos"])  # Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„
        if "dependency_analysis_optimization" in analysis["focus_areas"]:
            priority_functions.extend(["swish", "squared_relu"])  # Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
        if "structure_recognition_refinement" in analysis["focus_areas"]:
            priority_functions.extend(["hyperbolic", "softsign"])  # Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨
        if "complex_sentence_handling" in analysis["focus_areas"]:
            priority_functions.extend(["sin", "cos"])  # Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        if "relative_clause_analysis" in analysis["focus_areas"]:
            priority_functions.extend(["gaussian", "softplus"])  # Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©
        if "copular_sentence_analysis" in analysis["focus_areas"]:
            priority_functions.extend(["tanh", "swish"])  # Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù†Ø³ÙˆØ®Ø©

        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù†Ø­ÙˆÙŠ
        if analysis["complexity_assessment"] == "Ù†Ø­Ùˆ Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹":
            recommended_evolution = "increase"
            adaptation_strength = 0.95
        elif analysis["complexity_assessment"] == "Ù†Ø­Ùˆ Ù…ØªÙˆØ³Ø·":
            recommended_evolution = "restructure"
            adaptation_strength = 0.8
        else:
            recommended_evolution = "maintain"
            adaptation_strength = 0.7

        return MockSyntaxGuidance(
            target_complexity=target_complexity,
            focus_areas=analysis["focus_areas"],
            adaptation_strength=adaptation_strength,
            priority_functions=priority_functions or ["softplus", "gaussian"],
            recommended_evolution=recommended_evolution
        )

    def _adapt_syntax_equations(self, guidance, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ"""

        adaptations = {}

        # Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ ÙˆÙ‡Ù…ÙŠ Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ
        mock_analysis = MockSyntaxAnalysis(
            syntax_accuracy=0.5,
            parsing_clarity=0.6,
            pos_precision=0.65,
            dependency_coherence=0.55,
            areas_for_improvement=guidance.focus_areas
        )

        # ØªÙƒÙŠÙ ÙƒÙ„ Ù…Ø¹Ø§Ø¯Ù„Ø© Ù†Ø­Ùˆ
        for eq_name, equation in self.syntax_equations.items():
            print(f"   ğŸ”¤ ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ù†Ø­Ùˆ: {eq_name}")
            equation.adapt_with_expert_guidance(guidance, mock_analysis)
            adaptations[eq_name] = equation.get_expert_guidance_summary()

        return adaptations

    def _perform_adaptive_syntax_analysis(self, request: SyntaxAnalysisRequest, adaptations: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ù…ØªÙƒÙŠÙ"""

        analysis_results = {
            "sentence_type": "",
            "words_syntax": [],
            "grammatical_structure": {},
            "dependency_tree": {},
            "parsing_confidence": 0.0
        }

        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù…Ù„Ø©
        sentence_type = self._identify_sentence_type_adaptive(request.sentence)
        analysis_results["sentence_type"] = sentence_type

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù†Ø­ÙˆÙŠØ§Ù‹
        pos_accuracy = adaptations.get("pos_tagger", {}).get("pos_tagging_accuracy", 0.65)
        words_syntax = self._analyze_words_syntax_adaptive(request.sentence, pos_accuracy)
        analysis_results["words_syntax"] = words_syntax

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ
        structure_accuracy = adaptations.get("sentence_structure_detector", {}).get("sentence_structure_recognition", 0.6)
        grammatical_structure = self._analyze_grammatical_structure_adaptive(request.sentence, words_syntax, structure_accuracy)
        analysis_results["grammatical_structure"] = grammatical_structure

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
        dependency_accuracy = adaptations.get("dependency_parser", {}).get("dependency_accuracy", 0.55)
        dependency_tree = self._analyze_dependencies_adaptive(request.sentence, words_syntax, dependency_accuracy)
        analysis_results["dependency_tree"] = dependency_tree

        # Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        parsing_confidence = np.mean([pos_accuracy, structure_accuracy, dependency_accuracy])
        analysis_results["parsing_confidence"] = parsing_confidence

        return analysis_results

    def _identify_sentence_type_adaptive(self, sentence: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        # Ø¬Ù…Ù„Ø© ÙØ¹Ù„ÙŠØ©
        if any(sentence.strip().split()[0].endswith(suffix) for suffix in ["", "Øª", "Ù†Ø§", "ÙˆØ§", "ØªÙ…", "ØªÙ†"] if sentence.strip().split()):
            return "Ø¬Ù…Ù„Ø© ÙØ¹Ù„ÙŠØ©"

        # Ø¬Ù…Ù„Ø© Ø§Ø³Ù…ÙŠØ©
        if sentence.strip() and not any(sentence.strip().split()[0].endswith(suffix) for suffix in ["", "Øª", "Ù†Ø§", "ÙˆØ§", "ØªÙ…", "ØªÙ†"]):
            return "Ø¬Ù…Ù„Ø© Ø§Ø³Ù…ÙŠØ©"

        # Ø¬Ù…Ù„Ø© Ø´Ø±Ø·ÙŠØ©
        if any(word in sentence for word in ["Ø¥Ø°Ø§", "Ø¥Ù†", "Ù„Ùˆ", "Ù„ÙˆÙ„Ø§"]):
            return "Ø¬Ù…Ù„Ø© Ø´Ø±Ø·ÙŠØ©"

        # Ø¬Ù…Ù„Ø© Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©
        if any(word in sentence for word in ["Ù…Ø§", "Ù…Ù†", "Ù…ØªÙ‰", "Ø£ÙŠÙ†", "ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ù‡Ù„"]):
            return "Ø¬Ù…Ù„Ø© Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠØ©"

        # Ø¬Ù…Ù„Ø© Ù…Ù†Ø³ÙˆØ®Ø©
        if any(word in sentence for word in ["Ø¥Ù†", "Ø£Ù†", "ÙƒØ§Ù†", "Ø£ØµØ¨Ø­", "Ø¸Ù„", "Ø¨Ø§Øª"]):
            return "Ø¬Ù…Ù„Ø© Ù…Ù†Ø³ÙˆØ®Ø©"

        return "Ø¬Ù…Ù„Ø© Ø¨Ø³ÙŠØ·Ø©"

    def _analyze_words_syntax_adaptive(self, sentence: str, accuracy: float) -> List[WordSyntaxInfo]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù†Ø­ÙˆÙŠØ§Ù‹ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        words = sentence.split()
        words_syntax = []

        for i, word in enumerate(words):
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©
            pos_tag = self._determine_pos_tag_adaptive(word, i, words)

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©
            grammatical_case = self._determine_grammatical_case_adaptive(word, pos_tag, i, words)

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù†Ø­ÙˆÙŠØ©
            grammatical_function = self._determine_grammatical_function_adaptive(word, pos_tag, grammatical_case, i, words)

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
            dependencies = self._determine_dependencies_adaptive(word, i, words)

            word_syntax = WordSyntaxInfo(
                word=word,
                position=i,
                pos_tag=pos_tag,
                grammatical_case=grammatical_case,
                grammatical_function=grammatical_function,
                dependencies=dependencies,
                confidence=accuracy
            )

            words_syntax.append(word_syntax)

        return words_syntax

    def _determine_pos_tag_adaptive(self, word: str, position: int, words: List[str]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        # ÙØ¹Ù„ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ù„ØµÙŠØºØ©)
        if position == 0 and not word.startswith("Ø§Ù„"):
            return "ÙØ¹Ù„"

        # Ø§Ø³Ù… Ù…Ø¹Ø±Ù
        if word.startswith("Ø§Ù„"):
            return "Ø§Ø³Ù…"

        # Ø¶Ù…ÙŠØ±
        if word in ["Ù‡Ùˆ", "Ù‡ÙŠ", "Ù‡Ù…", "Ù‡Ù†", "Ø£Ù†Øª", "Ø£Ù†ØªÙ…", "Ø£Ù†Ø§", "Ù†Ø­Ù†"]:
            return "Ø¶Ù…ÙŠØ±"

        # Ø­Ø±Ù Ø¬Ø±
        if word in ["ÙÙŠ", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ù…Ù†", "Ø¹Ù†", "Ù…Ø¹", "Ø¨", "Ù„", "Ùƒ"]:
            return "Ø­Ø±Ù"

        # Ø­Ø±Ù Ù†ØµØ¨ Ø£Ùˆ Ø¬Ø²Ù…
        if word in ["Ø£Ù†", "Ù„Ù†", "Ù„Ù…", "Ù„Ø§", "Ø¥Ù†", "ÙƒØ§Ù†"]:
            return "Ø­Ø±Ù"

        # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù†Ù‡ Ø§Ø³Ù…
        return "Ø§Ø³Ù…"

    def _determine_grammatical_case_adaptive(self, word: str, pos_tag: str, position: int, words: List[str]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        if pos_tag == "Ø­Ø±Ù":
            return "Ù…Ø¨Ù†ÙŠ"

        # ÙØ§Ø¹Ù„ (Ù…Ø±ÙÙˆØ¹)
        if pos_tag == "Ø§Ø³Ù…" and position > 0 and words[position-1] not in ["ÙÙŠ", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ù…Ù†"]:
            return "Ø±ÙØ¹"

        # Ù…Ø¬Ø±ÙˆØ± Ø¨Ø­Ø±Ù Ø¬Ø±
        if position > 0 and words[position-1] in ["ÙÙŠ", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ù…Ù†", "Ø¹Ù†", "Ù…Ø¹", "Ø¨", "Ù„", "Ùƒ"]:
            return "Ø¬Ø±"

        # Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡ (Ù…Ù†ØµÙˆØ¨)
        if pos_tag == "Ø§Ø³Ù…" and position > 1:
            return "Ù†ØµØ¨"

        # Ø§ÙØªØ±Ø§Ø¶ Ø§Ù„Ø±ÙØ¹
        return "Ø±ÙØ¹"

    def _determine_grammatical_function_adaptive(self, word: str, pos_tag: str, case: str, position: int, words: List[str]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        if pos_tag == "ÙØ¹Ù„":
            return "ÙØ¹Ù„"

        if case == "Ø¬Ø±":
            return "Ù…Ø¬Ø±ÙˆØ±"

        if position == 0 and pos_tag == "Ø§Ø³Ù…":
            return "Ù…Ø¨ØªØ¯Ø£"

        if position == 1 and pos_tag == "Ø§Ø³Ù…" and words[0] not in ["ÙØ¹Ù„"]:
            return "Ø®Ø¨Ø±"

        if pos_tag == "Ø§Ø³Ù…" and case == "Ø±ÙØ¹" and position > 0:
            return "ÙØ§Ø¹Ù„"

        if pos_tag == "Ø§Ø³Ù…" and case == "Ù†ØµØ¨":
            return "Ù…ÙØ¹ÙˆÙ„_Ø¨Ù‡"

        return "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"

    def _determine_dependencies_adaptive(self, word: str, position: int, words: List[str]) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""
        dependencies = []

        # ØªØ¨Ø¹ÙŠØ© Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        if position > 0:
            dependencies.append(f"depends_on_{position-1}")

        # ØªØ¨Ø¹ÙŠØ© Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
        if position < len(words) - 1:
            dependencies.append(f"governs_{position+1}")

        return dependencies

    def _analyze_grammatical_structure_adaptive(self, sentence: str, words_syntax: List[WordSyntaxInfo], accuracy: float) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        structure = {
            "subject": None,
            "verb": None,
            "object": None,
            "modifiers": [],
            "phrases": []
        }

        for word_info in words_syntax:
            if word_info.grammatical_function == "ÙØ§Ø¹Ù„":
                structure["subject"] = word_info.word
            elif word_info.pos_tag == "ÙØ¹Ù„":
                structure["verb"] = word_info.word
            elif word_info.grammatical_function == "Ù…ÙØ¹ÙˆÙ„_Ø¨Ù‡":
                structure["object"] = word_info.word
            elif word_info.grammatical_function in ["Ù…Ø¬Ø±ÙˆØ±", "ØµÙØ©"]:
                structure["modifiers"].append(word_info.word)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª
        current_phrase = []
        for word_info in words_syntax:
            if word_info.pos_tag == "Ø­Ø±Ù" and current_phrase:
                structure["phrases"].append(" ".join(current_phrase))
                current_phrase = [word_info.word]
            else:
                current_phrase.append(word_info.word)

        if current_phrase:
            structure["phrases"].append(" ".join(current_phrase))

        return structure

    def _analyze_dependencies_adaptive(self, sentence: str, words_syntax: List[WordSyntaxInfo], accuracy: float) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""

        dependency_tree = {
            "root": None,
            "relations": []
        }

        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø°Ø± (Ø¹Ø§Ø¯Ø© Ø§Ù„ÙØ¹Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ)
        for word_info in words_syntax:
            if word_info.pos_tag == "ÙØ¹Ù„":
                dependency_tree["root"] = word_info.word
                break

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
        for i, word_info in enumerate(words_syntax):
            if word_info.pos_tag != "ÙØ¹Ù„":
                relation = {
                    "dependent": word_info.word,
                    "head": dependency_tree["root"] or words_syntax[0].word,
                    "relation_type": word_info.grammatical_function,
                    "confidence": accuracy
                }
                dependency_tree["relations"].append(relation)

        return dependency_tree

    def _measure_syntax_improvements(self, request: SyntaxAnalysisRequest, analysis: Dict[str, Any], adaptations: Dict[str, Any]) -> Dict[str, float]:
        """Ù‚ÙŠØ§Ø³ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø­Ùˆ"""

        improvements = {}

        # ØªØ­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„Ù†Ø­Ùˆ
        avg_syntax_accuracy = np.mean([adapt.get("syntax_accuracy", 0.5) for adapt in adaptations.values()])
        baseline_syntax_accuracy = 0.4
        syntax_accuracy_improvement = ((avg_syntax_accuracy - baseline_syntax_accuracy) / baseline_syntax_accuracy) * 100
        improvements["syntax_accuracy_improvement"] = max(0, syntax_accuracy_improvement)

        # ØªØ­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        avg_parsing_accuracy = np.mean([adapt.get("parsing_accuracy", 0.6) for adapt in adaptations.values()])
        baseline_parsing_accuracy = 0.5
        parsing_accuracy_improvement = ((avg_parsing_accuracy - baseline_parsing_accuracy) / baseline_parsing_accuracy) * 100
        improvements["parsing_accuracy_improvement"] = max(0, parsing_accuracy_improvement)

        # ØªØ­Ø³Ù† ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        avg_pos_tagging = np.mean([adapt.get("pos_tagging_accuracy", 0.65) for adapt in adaptations.values()])
        baseline_pos_tagging = 0.55
        pos_tagging_improvement = ((avg_pos_tagging - baseline_pos_tagging) / baseline_pos_tagging) * 100
        improvements["pos_tagging_improvement"] = max(0, pos_tagging_improvement)

        # ØªØ­Ø³Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
        avg_dependency_accuracy = np.mean([adapt.get("dependency_accuracy", 0.55) for adapt in adaptations.values()])
        baseline_dependency_accuracy = 0.45
        dependency_accuracy_improvement = ((avg_dependency_accuracy - baseline_dependency_accuracy) / baseline_dependency_accuracy) * 100
        improvements["dependency_accuracy_improvement"] = max(0, dependency_accuracy_improvement)

        # ØªØ­Ø³Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ
        avg_grammatical_analysis = np.mean([adapt.get("grammatical_analysis", 0.5) for adapt in adaptations.values()])
        baseline_grammatical_analysis = 0.4
        grammatical_analysis_improvement = ((avg_grammatical_analysis - baseline_grammatical_analysis) / baseline_grammatical_analysis) * 100
        improvements["grammatical_analysis_improvement"] = max(0, grammatical_analysis_improvement)

        # ØªØ­Ø³Ù† ØªÙ…ÙŠÙŠØ² Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨
        avg_structure_recognition = np.mean([adapt.get("sentence_structure_recognition", 0.6) for adapt in adaptations.values()])
        baseline_structure_recognition = 0.5
        structure_recognition_improvement = ((avg_structure_recognition - baseline_structure_recognition) / baseline_structure_recognition) * 100
        improvements["structure_recognition_improvement"] = max(0, structure_recognition_improvement)

        # ØªØ­Ø³Ù† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ø­ÙˆÙŠ
        total_adaptations = sum(adapt.get("total_adaptations", 0) for adapt in adaptations.values())
        syntax_complexity_improvement = total_adaptations * 15  # ÙƒÙ„ ØªÙƒÙŠÙ Ù†Ø­ÙˆÙŠ = 15% ØªØ­Ø³Ù†
        improvements["syntax_complexity_improvement"] = syntax_complexity_improvement

        return improvements

    def _extract_syntax_learning_insights(self, request: SyntaxAnalysisRequest, analysis: Dict[str, Any], improvements: Dict[str, float]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø­ÙˆÙŠ"""

        insights = []

        if improvements["syntax_accuracy_improvement"] > 25:
            insights.append("Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±")

        if improvements["parsing_accuracy_improvement"] > 20:
            insights.append("Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ù…Ù…ØªØ§Ø²Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

        if improvements["pos_tagging_improvement"] > 18:
            insights.append("Ø§Ù„Ù†Ø¸Ø§Ù… Ù†Ø¬Ø­ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª")

        if improvements["dependency_accuracy_improvement"] > 22:
            insights.append("ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ© ØªØ­Ø³Ù† Ù…Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ±")

        if improvements["grammatical_analysis_improvement"] > 25:
            insights.append("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø£ØµØ¨Ø­ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„ØªÙƒÙŠÙ")

        if improvements["structure_recognition_improvement"] > 20:
            insights.append("ØªÙ…ÙŠÙŠØ² Ø§Ù„ØªØ±Ø§ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠØ© ØªØ­Ø³Ù† Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø­ÙˆØ¸")

        if improvements["syntax_complexity_improvement"] > 100:
            insights.append("Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø§Ù„Ù…ØªÙƒÙŠÙØ© ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø¨ÙƒÙØ§Ø¡Ø©")

        # Ø±Ø¤Ù‰ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¬Ù…Ù„Ø©
        words_count = len(request.sentence.split())
        if words_count > 8:
            insights.append("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØªØ¹Ø§Ù…Ù„ Ø¨ÙƒÙØ§Ø¡Ø© Ù…Ø¹ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")

        if request.context:
            insights.append("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ ÙŠØ­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ")

        if analysis.get("sentence_type") == "Ø¬Ù…Ù„Ø© Ù…Ù†Ø³ÙˆØ®Ø©":
            insights.append("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ­Ù„Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù†Ø³ÙˆØ®Ø© Ø¨Ø¯Ù‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø©")

        return insights

    def _generate_syntax_next_cycle_recommendations(self, improvements: Dict[str, float], insights: List[str]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©"""

        recommendations = []

        avg_improvement = np.mean(list(improvements.values()))

        if avg_improvement > 35:
            recommendations.append("Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠØ©")
            recommendations.append("ØªØ¬Ø±Ø¨Ø© ØªØ­Ù„ÙŠÙ„ Ù†Ø­ÙˆÙŠ Ø£Ø¹Ù…Ù‚ Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")
        elif avg_improvement > 20:
            recommendations.append("Ø²ÙŠØ§Ø¯Ø© Ù‚ÙˆØ© Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù†Ø­ÙˆÙŠ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹")
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ù‚ÙˆØ§Ø¹Ø¯ Ù†Ø­ÙˆÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©")
        else:
            recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ø­ÙˆÙŠ")
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø­Ùˆ")

        # ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø©
        if "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª" in str(insights):
            recommendations.append("Ø§Ù„ØªÙˆØ³Ø¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

        if "Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª" in str(insights):
            recommendations.append("ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©")

        if "Ù…Ø¹Ù‚Ø¯Ø©" in str(insights):
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„Ù…Ø±ÙƒØ¨Ø©")

        if "Ù…Ù†Ø³ÙˆØ®Ø©" in str(insights):
            recommendations.append("ØªØ¹Ø²ÙŠØ² ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ù†Ø³ÙˆØ®Ø© ÙˆØ§Ù„Ø´Ø±Ø·ÙŠØ©")

        return recommendations

    def _save_syntax_learning(self, request: SyntaxAnalysisRequest, result: SyntaxAnalysisResult):
        """Ø­ÙØ¸ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù†Ø­ÙˆÙŠ"""

        learning_entry = {
            "timestamp": datetime.now().isoformat(),
            "sentence": request.sentence,
            "context": request.context,
            "analysis_depth": request.analysis_depth,
            "success": result.success,
            "sentence_type": result.sentence_type,
            "parsing_confidence": result.parsing_confidence,
            "performance_improvements": result.performance_improvements,
            "learning_insights": result.learning_insights
        }

        sentence_key = f"{len(request.sentence.split())}_{request.analysis_depth}"
        if sentence_key not in self.syntax_learning_database:
            self.syntax_learning_database[sentence_key] = []

        self.syntax_learning_database[sentence_key].append(learning_entry)

        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 3 Ø¥Ø¯Ø®Ø§Ù„Ø§Øª ÙÙ‚Ø·
        if len(self.syntax_learning_database[sentence_key]) > 3:
            self.syntax_learning_database[sentence_key] = self.syntax_learning_database[sentence_key][-3:]

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±...")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù†Ø­ÙˆÙŠ
    syntax_analyzer = ExpertGuidedArabicSyntaxAnalyzer()

    # Ø¬Ù…Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø±Ø¨ÙŠØ©
    test_sentences = [
        "Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠÙƒØªØ¨ Ø§Ù„Ø¯Ø±Ø³",
        "ÙƒØªØ¨ Ø§Ù„Ù…Ø¹Ù„Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¨ÙˆØ±Ø©",
        "Ø¥Ù† Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©",
        "Ù‡Ù„ ØªØ¹Ø±Ù Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙƒØªØ¨Ø©ØŸ",
        "Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø°ÙŠ Ù‚Ø±Ø£ØªÙ‡ Ù…ÙÙŠØ¯ Ø¬Ø¯Ø§Ù‹"
    ]

    for sentence in test_sentences:
        print(f"\n{'='*60}")
        print(f"ğŸ”¤ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©: {sentence}")

        # Ø·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ
        syntax_request = SyntaxAnalysisRequest(
            sentence=sentence,
            context="Ø³ÙŠØ§Ù‚ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø¬Ù…Ù„Ø©",
            analysis_depth="comprehensive",
            syntax_aspects=["pos", "parsing", "dependencies", "structure"],
            expert_guidance_level="adaptive",
            learning_enabled=True
        )

        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ
        syntax_result = syntax_analyzer.analyze_syntax_with_expert_guidance(syntax_request)

        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ø­ÙˆÙŠØ©
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ:")
        print(f"   âœ… Ø§Ù„Ù†Ø¬Ø§Ø­: {syntax_result.success}")
        print(f"   ğŸ“ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù…Ù„Ø©: {syntax_result.sentence_type}")
        print(f"   ğŸ¯ Ø«Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {syntax_result.parsing_confidence:.2%}")

        print(f"   ğŸ”¤ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª:")
        for word_info in syntax_result.words_syntax:
            print(f"      {word_info.word}: {word_info.pos_tag} | {word_info.grammatical_case} | {word_info.grammatical_function}")

        if syntax_result.grammatical_structure:
            print(f"   ğŸ—ï¸ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ø­ÙˆÙŠ:")
            structure = syntax_result.grammatical_structure
            if structure.get("subject"):
                print(f"      Ø§Ù„ÙØ§Ø¹Ù„: {structure['subject']}")
            if structure.get("verb"):
                print(f"      Ø§Ù„ÙØ¹Ù„: {structure['verb']}")
            if structure.get("object"):
                print(f"      Ø§Ù„Ù…ÙØ¹ÙˆÙ„: {structure['object']}")

        if syntax_result.performance_improvements:
            print(f"   ğŸ“ˆ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:")
            for metric, improvement in syntax_result.performance_improvements.items():
                print(f"      {metric}: {improvement:.1f}%")

        if syntax_result.learning_insights:
            print(f"   ğŸ§  Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù…:")
            for insight in syntax_result.learning_insights:
                print(f"      â€¢ {insight}")

if __name__ == "__main__":
    main()
