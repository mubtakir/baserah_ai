#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Expert-Guided Arabic Morphology Analyzer - Part 1: Morphological Analysis
Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ

Revolutionary integration of Expert/Explorer guidance with Arabic morphological analysis,
applying adaptive mathematical equations to achieve superior morphological understanding.

Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ
ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ù„ØªØ­Ù‚ÙŠÙ‚ ÙÙ‡Ù… ØµØ±ÙÙŠ Ù…ØªÙÙˆÙ‚.

Author: Basil Yahya Abdullah - Iraq/Mosul
Version: 1.0.0 - REVOLUTIONARY ARABIC MORPHOLOGY
"""

import numpy as np
import sys
import os
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
import re

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒÙŠÙ Ù„Ù„ØµØ±Ù
class MockMorphologyEquation:
    def __init__(self, name: str, input_dim: int, output_dim: int):
        self.name = name
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.current_complexity = 8  # Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹Ù‚Ø¯
        self.adaptation_count = 0
        self.morphology_accuracy = 0.6  # Ø¯Ù‚Ø© ØµØ±ÙÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©
        self.root_extraction_accuracy = 0.7
        self.pattern_recognition = 0.65
        self.affix_analysis = 0.6
        self.stem_identification = 0.55

    def adapt_with_expert_guidance(self, guidance, analysis):
        self.adaptation_count += 1
        if hasattr(guidance, 'recommended_evolution'):
            if guidance.recommended_evolution == "increase":
                self.current_complexity += 3
                self.morphology_accuracy += 0.05
                self.root_extraction_accuracy += 0.03
                self.pattern_recognition += 0.04
                self.affix_analysis += 0.02
                self.stem_identification += 0.03
            elif guidance.recommended_evolution == "restructure":
                self.morphology_accuracy += 0.02
                self.root_extraction_accuracy += 0.01
                self.pattern_recognition += 0.02

    def get_expert_guidance_summary(self):
        return {
            "current_complexity": self.current_complexity,
            "total_adaptations": self.adaptation_count,
            "morphology_accuracy": self.morphology_accuracy,
            "root_extraction_accuracy": self.root_extraction_accuracy,
            "pattern_recognition": self.pattern_recognition,
            "affix_analysis": self.affix_analysis,
            "stem_identification": self.stem_identification,
            "average_improvement": 0.03 * self.adaptation_count
        }

class MockMorphologyGuidance:
    def __init__(self, target_complexity, focus_areas, adaptation_strength, priority_functions, recommended_evolution):
        self.target_complexity = target_complexity
        self.focus_areas = focus_areas
        self.adaptation_strength = adaptation_strength
        self.priority_functions = priority_functions
        self.recommended_evolution = recommended_evolution

class MockMorphologyAnalysis:
    def __init__(self, morphology_accuracy, root_clarity, pattern_coherence, affix_precision, areas_for_improvement):
        self.morphology_accuracy = morphology_accuracy
        self.root_clarity = root_clarity
        self.pattern_coherence = pattern_coherence
        self.affix_precision = affix_precision
        self.areas_for_improvement = areas_for_improvement

@dataclass
class MorphologyAnalysisRequest:
    """Ø·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ"""
    word: str
    context: str = ""
    analysis_depth: str = "comprehensive"  # "basic", "intermediate", "comprehensive"
    morphology_aspects: List[str] = None  # ["root", "pattern", "affixes", "stem"]
    expert_guidance_level: str = "adaptive"
    learning_enabled: bool = True

@dataclass
class MorphologyAnalysisResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ"""
    success: bool
    word: str
    root: str
    pattern: str
    stem: str
    prefixes: List[str]
    suffixes: List[str]
    morphological_features: Dict[str, Any]
    confidence_scores: Dict[str, float]
    expert_guidance_applied: Dict[str, Any] = None
    equation_adaptations: Dict[str, Any] = None
    performance_improvements: Dict[str, float] = None
    learning_insights: List[str] = None
    next_cycle_recommendations: List[str] = None

class ExpertGuidedArabicMorphologyAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø«ÙˆØ±ÙŠ"""

    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
        print("ğŸŒŸ" + "="*100 + "ğŸŒŸ")
        print("ğŸ“š Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø«ÙˆØ±ÙŠ")
        print("ğŸ” Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù ÙŠÙ‚ÙˆØ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø°ÙƒØ§Ø¡")
        print("ğŸ§® Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ù…ØªÙƒÙŠÙØ© + ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ Ù…ØªÙ‚Ø¯Ù…")
        print("ğŸŒŸ Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù…Ù† Ø§Ù„Ø¹Ø±Ø§Ù‚/Ø§Ù„Ù…ÙˆØµÙ„ ğŸŒŸ")
        print("ğŸŒŸ" + "="*100 + "ğŸŒŸ")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…ØªØ®ØµØµØ©
        self.morphology_equations = {
            "root_extractor": MockMorphologyEquation("root_extraction", 20, 15),
            "pattern_analyzer": MockMorphologyEquation("pattern_analysis", 18, 12),
            "affix_detector": MockMorphologyEquation("affix_detection", 15, 10),
            "stem_identifier": MockMorphologyEquation("stem_identification", 16, 12),
            "morpheme_segmenter": MockMorphologyEquation("morpheme_segmentation", 22, 18),
            "feature_extractor": MockMorphologyEquation("feature_extraction", 25, 20),
            "pattern_matcher": MockMorphologyEquation("pattern_matching", 20, 15),
            "derivation_analyzer": MockMorphologyEquation("derivation_analysis", 24, 18)
        }

        # Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        self.morphology_laws = {
            "root_preservation": {
                "name": "Ø­ÙØ¸ Ø§Ù„Ø¬Ø°Ø±",
                "description": "Ø§Ù„Ø¬Ø°Ø± ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù…Ø¹Ù†Ø§Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª",
                "formula": "Root(word) = Core_Meaning(word)"
            },
            "pattern_consistency": {
                "name": "Ø«Ø¨Ø§Øª Ø§Ù„ÙˆØ²Ù†",
                "description": "Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ ÙŠØ­Ø¯Ø¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© ÙˆÙˆØ¸ÙŠÙØªÙ‡Ø§",
                "formula": "Pattern(word) â†’ Function(word)"
            },
            "affix_harmony": {
                "name": "ØªÙ†Ø§ØºÙ… Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯",
                "description": "Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯ ØªØªÙ†Ø§ØºÙ… Ù…Ø¹ Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„ÙˆØ²Ù†",
                "formula": "Affix + Root + Pattern = Harmonious_Word"
            }
        }

        # Ø«ÙˆØ§Ø¨Øª Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        self.morphology_constants = {
            "root_strength": 0.8,
            "pattern_weight": 0.7,
            "affix_influence": 0.6,
            "derivation_factor": 0.75,
            "morpheme_coherence": 0.85
        }

        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.arabic_roots = self._load_arabic_roots()

        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.arabic_patterns = self._load_arabic_patterns()

        # ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©
        self.morphology_history = []
        self.morphology_learning_database = {}

        print("ğŸ“š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØªØ®ØµØµØ©:")
        for eq_name in self.morphology_equations.keys():
            print(f"   âœ… {eq_name}")

        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±!")

    def _load_arabic_roots(self) -> Dict[str, Dict[str, Any]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        # Ø¬Ø°ÙˆØ± Ø¹Ø±Ø¨ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø¨Ø³Ø·Ø© Ù„Ù„ØªØ¬Ø±Ø¨Ø©)
        return {
            "ÙƒØªØ¨": {"meaning": "Ø§Ù„ÙƒØªØ§Ø¨Ø©", "type": "ÙØ¹Ù„", "derivatives": ["ÙƒØ§ØªØ¨", "Ù…ÙƒØªÙˆØ¨", "ÙƒØªØ§Ø¨"]},
            "Ù‚Ø±Ø£": {"meaning": "Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©", "type": "ÙØ¹Ù„", "derivatives": ["Ù‚Ø§Ø±Ø¦", "Ù…Ù‚Ø±ÙˆØ¡", "Ù‚Ø±Ø§Ø¡Ø©"]},
            "Ø¹Ù„Ù…": {"meaning": "Ø§Ù„Ø¹Ù„Ù…", "type": "Ø§Ø³Ù…/ÙØ¹Ù„", "derivatives": ["Ø¹Ø§Ù„Ù…", "Ù…Ø¹Ù„ÙˆÙ…", "ØªØ¹Ù„ÙŠÙ…"]},
            "Ø­ÙƒÙ…": {"meaning": "Ø§Ù„Ø­ÙƒÙ…", "type": "ÙØ¹Ù„", "derivatives": ["Ø­Ø§ÙƒÙ…", "Ù…Ø­ÙƒÙˆÙ…", "Ø­ÙƒÙ…Ø©"]},
            "Ø³Ù„Ù…": {"meaning": "Ø§Ù„Ø³Ù„Ø§Ù…", "type": "Ø§Ø³Ù…/ÙØ¹Ù„", "derivatives": ["Ø³Ø§Ù„Ù…", "Ù…Ø³Ù„Ù…", "Ø³Ù„Ø§Ù…Ø©"]},
            "Ø±Ø­Ù…": {"meaning": "Ø§Ù„Ø±Ø­Ù…Ø©", "type": "ÙØ¹Ù„", "derivatives": ["Ø±Ø§Ø­Ù…", "Ù…Ø±Ø­ÙˆÙ…", "Ø±Ø­Ù…Ø©"]},
            "ØµØ¨Ø±": {"meaning": "Ø§Ù„ØµØ¨Ø±", "type": "ÙØ¹Ù„", "derivatives": ["ØµØ§Ø¨Ø±", "Ù…ØµØ¨ÙˆØ±", "ØµØ¨Ø±"]},
            "Ø´ÙƒØ±": {"meaning": "Ø§Ù„Ø´ÙƒØ±", "type": "ÙØ¹Ù„", "derivatives": ["Ø´Ø§ÙƒØ±", "Ù…Ø´ÙƒÙˆØ±", "Ø´ÙƒØ±"]}
        }

    def _load_arabic_patterns(self) -> Dict[str, Dict[str, Any]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        return {
            "ÙØ¹Ù„": {"type": "ÙØ¹Ù„ Ù…Ø§Ø¶ÙŠ", "example": "ÙƒØªØ¨", "function": "Ø­Ø¯Ø« ÙÙŠ Ø§Ù„Ù…Ø§Ø¶ÙŠ"},
            "ÙŠÙØ¹Ù„": {"type": "ÙØ¹Ù„ Ù…Ø¶Ø§Ø±Ø¹", "example": "ÙŠÙƒØªØ¨", "function": "Ø­Ø¯Ø« ÙÙŠ Ø§Ù„Ø­Ø§Ø¶Ø±/Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„"},
            "ÙØ§Ø¹Ù„": {"type": "Ø§Ø³Ù… ÙØ§Ø¹Ù„", "example": "ÙƒØ§ØªØ¨", "function": "Ù…Ù† ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„ÙØ¹Ù„"},
            "Ù…ÙØ¹ÙˆÙ„": {"type": "Ø§Ø³Ù… Ù…ÙØ¹ÙˆÙ„", "example": "Ù…ÙƒØªÙˆØ¨", "function": "Ù…Ø§ ÙˆÙ‚Ø¹ Ø¹Ù„ÙŠÙ‡ Ø§Ù„ÙØ¹Ù„"},
            "ÙØ¹Ø§Ù„": {"type": "ØµÙŠØºØ© Ù…Ø¨Ø§Ù„ØºØ©", "example": "ÙƒØªØ§Ø¨", "function": "Ø§Ù„Ù…Ø¨Ø§Ù„ØºØ© ÙÙŠ Ø§Ù„ÙˆØµÙ"},
            "ØªÙØ¹ÙŠÙ„": {"type": "Ù…ØµØ¯Ø±", "example": "ØªÙƒØªÙŠØ¨", "function": "Ø§Ø³Ù… Ø§Ù„Ø­Ø¯Ø«"},
            "Ø§Ø³ØªÙØ¹Ø§Ù„": {"type": "Ø§Ø³ØªÙØ¹Ø§Ù„", "example": "Ø§Ø³ØªÙƒØªØ§Ø¨", "function": "Ø·Ù„Ø¨ Ø§Ù„ÙØ¹Ù„"},
            "Ø§Ù†ÙØ¹Ø§Ù„": {"type": "Ø§Ù†ÙØ¹Ø§Ù„", "example": "Ø§Ù†ÙƒØªØ§Ø¨", "function": "Ø§Ù„ØªØ£Ø«Ø± Ø¨Ø§Ù„ÙØ¹Ù„"}
        }

    def analyze_morphology_with_expert_guidance(self, request: MorphologyAnalysisRequest) -> MorphologyAnalysisResult:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
        print(f"\nğŸ“š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„ÙƒÙ„Ù…Ø©: {request.word}")
        start_time = datetime.now()

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„Ø·Ù„Ø¨ Ø§Ù„ØµØ±ÙÙŠ
        expert_analysis = self._analyze_morphology_request_with_expert(request)
        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„ØµØ±ÙÙŠ: {expert_analysis['complexity_assessment']}")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù
        expert_guidance = self._generate_morphology_expert_guidance(request, expert_analysis)
        print(f"ğŸ“š ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„ØµØ±ÙÙŠ: {expert_guidance.recommended_evolution}")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù
        equation_adaptations = self._adapt_morphology_equations(expert_guidance, expert_analysis)
        print(f"ğŸ§® ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù: {len(equation_adaptations)} Ù…Ø¹Ø§Ø¯Ù„Ø©")

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ØªÙƒÙŠÙ
        morphology_analysis = self._perform_adaptive_morphology_analysis(request, equation_adaptations)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©
        performance_improvements = self._measure_morphology_improvements(request, morphology_analysis, equation_adaptations)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 6: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØµØ±ÙÙŠ
        learning_insights = self._extract_morphology_learning_insights(request, morphology_analysis, performance_improvements)

        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 7: ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
        next_cycle_recommendations = self._generate_morphology_next_cycle_recommendations(performance_improvements, learning_insights)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ØµØ±ÙÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        result = MorphologyAnalysisResult(
            success=True,
            word=request.word,
            root=morphology_analysis.get("root", ""),
            pattern=morphology_analysis.get("pattern", ""),
            stem=morphology_analysis.get("stem", ""),
            prefixes=morphology_analysis.get("prefixes", []),
            suffixes=morphology_analysis.get("suffixes", []),
            morphological_features=morphology_analysis.get("features", {}),
            confidence_scores=morphology_analysis.get("confidence_scores", {}),
            expert_guidance_applied=expert_guidance.__dict__,
            equation_adaptations=equation_adaptations,
            performance_improvements=performance_improvements,
            learning_insights=learning_insights,
            next_cycle_recommendations=next_cycle_recommendations
        )

        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØµØ±ÙÙŠ
        self._save_morphology_learning(request, result)

        total_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ ÙÙŠ {total_time:.2f} Ø«Ø§Ù†ÙŠØ©")

        return result

    def _analyze_morphology_request_with_expert(self, request: MorphologyAnalysisRequest) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø·Ù„Ø¨ Ø§Ù„ØµØ±Ù Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø®Ø¨ÙŠØ±"""

        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„ÙƒÙ„Ù…Ø©
        word_complexity = len(request.word) * 0.5
        context_complexity = len(request.context.split()) * 0.3 if request.context else 0

        # ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„ØµØ±Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        aspects = request.morphology_aspects or ["root", "pattern", "affixes", "stem"]
        aspects_complexity = len(aspects) * 2.0

        # ØªØ­Ù„ÙŠÙ„ Ø¹Ù…Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        depth_complexity = {
            "basic": 2.0,
            "intermediate": 4.0,
            "comprehensive": 6.0
        }.get(request.analysis_depth, 4.0)

        total_complexity = word_complexity + context_complexity + aspects_complexity + depth_complexity

        return {
            "word_complexity": word_complexity,
            "context_complexity": context_complexity,
            "aspects_complexity": aspects_complexity,
            "depth_complexity": depth_complexity,
            "total_complexity": total_complexity,
            "complexity_assessment": "ØµØ±Ù Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹" if total_complexity > 15 else "ØµØ±Ù Ù…ØªÙˆØ³Ø·" if total_complexity > 8 else "ØµØ±Ù Ø¨Ø³ÙŠØ·",
            "recommended_adaptations": int(total_complexity // 2) + 3,
            "focus_areas": self._identify_morphology_focus_areas(request)
        }

    def _identify_morphology_focus_areas(self, request: MorphologyAnalysisRequest) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„ØµØ±ÙÙŠ"""
        focus_areas = []

        aspects = request.morphology_aspects or ["root", "pattern", "affixes", "stem"]

        if "root" in aspects:
            focus_areas.append("root_extraction_enhancement")
        if "pattern" in aspects:
            focus_areas.append("pattern_recognition_improvement")
        if "affixes" in aspects:
            focus_areas.append("affix_analysis_optimization")
        if "stem" in aspects:
            focus_areas.append("stem_identification_refinement")

        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„ÙƒÙ„Ù…Ø©
        if len(request.word) > 6:
            focus_areas.append("complex_word_handling")
        if any(char in request.word for char in "Ø§Ù„"):
            focus_areas.append("definite_article_processing")
        if request.context:
            focus_areas.append("contextual_morphology_analysis")

        return focus_areas

    def _generate_morphology_expert_guidance(self, request: MorphologyAnalysisRequest, analysis: Dict[str, Any]):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ"""

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„ØµØ±Ù
        target_complexity = 10 + analysis["recommended_adaptations"]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        priority_functions = []
        if "root_extraction_enhancement" in analysis["focus_areas"]:
            priority_functions.extend(["tanh", "softplus"])  # Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        if "pattern_recognition_improvement" in analysis["focus_areas"]:
            priority_functions.extend(["sin_cos", "gaussian"])  # Ù„ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        if "affix_analysis_optimization" in analysis["focus_areas"]:
            priority_functions.extend(["squared_relu", "swish"])  # Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯
        if "stem_identification_refinement" in analysis["focus_areas"]:
            priority_functions.extend(["hyperbolic", "softsign"])  # Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø°Ø¹
        if "complex_word_handling" in analysis["focus_areas"]:
            priority_functions.extend(["sin", "cos"])  # Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        if "contextual_morphology_analysis" in analysis["focus_areas"]:
            priority_functions.extend(["gaussian", "softplus"])  # Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ

        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØµØ±ÙÙŠ
        if analysis["complexity_assessment"] == "ØµØ±Ù Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹":
            recommended_evolution = "increase"
            adaptation_strength = 0.9
        elif analysis["complexity_assessment"] == "ØµØ±Ù Ù…ØªÙˆØ³Ø·":
            recommended_evolution = "restructure"
            adaptation_strength = 0.7
        else:
            recommended_evolution = "maintain"
            adaptation_strength = 0.6

        return MockMorphologyGuidance(
            target_complexity=target_complexity,
            focus_areas=analysis["focus_areas"],
            adaptation_strength=adaptation_strength,
            priority_functions=priority_functions or ["tanh", "gaussian"],
            recommended_evolution=recommended_evolution
        )

    def _adapt_morphology_equations(self, guidance, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù"""

        adaptations = {}

        # Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ ÙˆÙ‡Ù…ÙŠ Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù
        mock_analysis = MockMorphologyAnalysis(
            morphology_accuracy=0.6,
            root_clarity=0.7,
            pattern_coherence=0.65,
            affix_precision=0.6,
            areas_for_improvement=guidance.focus_areas
        )

        # ØªÙƒÙŠÙ ÙƒÙ„ Ù…Ø¹Ø§Ø¯Ù„Ø© ØµØ±Ù
        for eq_name, equation in self.morphology_equations.items():
            print(f"   ğŸ“š ØªÙƒÙŠÙ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ØµØ±Ù: {eq_name}")
            equation.adapt_with_expert_guidance(guidance, mock_analysis)
            adaptations[eq_name] = equation.get_expert_guidance_summary()

        return adaptations

    def _perform_adaptive_morphology_analysis(self, request: MorphologyAnalysisRequest, adaptations: Dict[str, Any]) -> Dict[str, Any]:
        """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ØªÙƒÙŠÙ"""

        analysis_results = {
            "root": "",
            "pattern": "",
            "stem": "",
            "prefixes": [],
            "suffixes": [],
            "features": {},
            "confidence_scores": {}
        }

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        root_accuracy = adaptations.get("root_extractor", {}).get("root_extraction_accuracy", 0.7)
        extracted_root = self._extract_root_adaptive(request.word, root_accuracy)
        analysis_results["root"] = extracted_root
        analysis_results["confidence_scores"]["root"] = root_accuracy

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù†
        pattern_accuracy = adaptations.get("pattern_analyzer", {}).get("pattern_recognition", 0.65)
        identified_pattern = self._identify_pattern_adaptive(request.word, extracted_root, pattern_accuracy)
        analysis_results["pattern"] = identified_pattern
        analysis_results["confidence_scores"]["pattern"] = pattern_accuracy

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯
        affix_accuracy = adaptations.get("affix_detector", {}).get("affix_analysis", 0.6)
        prefixes, suffixes = self._analyze_affixes_adaptive(request.word, affix_accuracy)
        analysis_results["prefixes"] = prefixes
        analysis_results["suffixes"] = suffixes
        analysis_results["confidence_scores"]["affixes"] = affix_accuracy

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø°Ø¹
        stem_accuracy = adaptations.get("stem_identifier", {}).get("stem_identification", 0.55)
        identified_stem = self._identify_stem_adaptive(request.word, prefixes, suffixes, stem_accuracy)
        analysis_results["stem"] = identified_stem
        analysis_results["confidence_scores"]["stem"] = stem_accuracy

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©
        features = self._extract_morphological_features(request.word, extracted_root, identified_pattern)
        analysis_results["features"] = features

        return analysis_results

    def _extract_root_adaptive(self, word: str, accuracy: float) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø£ÙˆÙ„Ø§Ù‹
        cleaned_word = word

        # Ø¥Ø²Ø§Ù„Ø© Ø£Ù„ Ø§Ù„ØªØ¹Ø±ÙŠÙ
        if cleaned_word.startswith("Ø§Ù„"):
            cleaned_word = cleaned_word[2:]

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø£Ø®Ø±Ù‰
        common_prefixes = ["Ùˆ", "Ù", "Ø¨", "Ù„", "Ù„Ù„", "Ø¨Ø§Ù„", "ÙØ§Ù„", "ÙˆØ§Ù„"]
        for prefix in common_prefixes:
            if cleaned_word.startswith(prefix):
                cleaned_word = cleaned_word[len(prefix):]
                break

        common_suffixes = ["Ø©", "Ø§Øª", "ÙˆÙ†", "ÙŠÙ†", "Ø§Ù†", "Ù‡", "Ù‡Ø§", "Ù‡Ù…", "Ù‡Ù†", "Ùƒ", "ÙƒÙ…", "ÙƒÙ†", "ÙŠ", "Ù†Ø§", "Ù†ÙŠ", "ØªÙ…", "ØªÙ†"]
        for suffix in common_suffixes:
            if cleaned_word.endswith(suffix):
                cleaned_word = cleaned_word[:-len(suffix)]
                break

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for root in self.arabic_roots:
            if root in cleaned_word or cleaned_word in root:
                return root

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ù†Ø¸ÙØ©
        if len(cleaned_word) >= 3:
            return cleaned_word[:3]  # Ø£Ø®Ø° Ø£ÙˆÙ„ 3 Ø£Ø­Ø±Ù ÙƒØ¬Ø°Ø± Ù…Ø¤Ù‚Øª

        return cleaned_word

    def _identify_pattern_adaptive(self, word: str, root: str, accuracy: float) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù† Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""
        if not root:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ ÙˆØ²Ù†
        pattern = word

        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø­Ø±Ù Ø§Ù„Ø¬Ø°Ø± Ø¨Ù€ Ù Ø¹ Ù„
        if len(root) >= 3:
            root_chars = list(root)
            pattern = pattern.replace(root_chars[0], "Ù")
            if len(root_chars) > 1:
                pattern = pattern.replace(root_chars[1], "Ø¹")
            if len(root_chars) > 2:
                pattern = pattern.replace(root_chars[2], "Ù„")

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        for pattern_key in self.arabic_patterns:
            if pattern_key in pattern or pattern in pattern_key:
                return pattern_key

        return pattern

    def _analyze_affixes_adaptive(self, word: str, accuracy: float) -> Tuple[List[str], List[str]]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""
        prefixes = []
        suffixes = []

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
        common_prefixes = ["Ø§Ù„", "Ùˆ", "Ù", "Ø¨", "Ù„", "Ù„Ù„", "Ø¨Ø§Ù„", "ÙØ§Ù„", "ÙˆØ§Ù„", "ÙƒØ§Ù„"]
        for prefix in common_prefixes:
            if word.startswith(prefix):
                prefixes.append(prefix)
                break

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        common_suffixes = ["Ø©", "Ø§Øª", "ÙˆÙ†", "ÙŠÙ†", "Ø§Ù†", "Ù‡", "Ù‡Ø§", "Ù‡Ù…", "Ù‡Ù†", "Ùƒ", "ÙƒÙ…", "ÙƒÙ†", "ÙŠ", "Ù†Ø§", "Ù†ÙŠ", "ØªÙ…", "ØªÙ†", "ØªÙ…Ø§", "ØªØ§Ù†"]
        for suffix in common_suffixes:
            if word.endswith(suffix):
                suffixes.append(suffix)
                break

        return prefixes, suffixes

    def _identify_stem_adaptive(self, word: str, prefixes: List[str], suffixes: List[str], accuracy: float) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø°Ø¹ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙƒÙŠÙØ©"""
        stem = word

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
        for prefix in prefixes:
            if stem.startswith(prefix):
                stem = stem[len(prefix):]

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        for suffix in suffixes:
            if stem.endswith(suffix):
                stem = stem[:-len(suffix)]

        return stem

    def _extract_morphological_features(self, word: str, root: str, pattern: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµØ±ÙÙŠØ©"""
        features = {
            "word_length": len(word),
            "root_length": len(root) if root else 0,
            "pattern_type": self.arabic_patterns.get(pattern, {}).get("type", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
            "has_definite_article": word.startswith("Ø§Ù„"),
            "has_conjunction": word.startswith("Ùˆ"),
            "estimated_complexity": len(word) + (len(root) if root else 0)
        }

        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ù† Ø§Ù„ÙˆØ²Ù†
        if pattern in self.arabic_patterns:
            pattern_info = self.arabic_patterns[pattern]
            features["word_type"] = pattern_info.get("type", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
            features["function"] = pattern_info.get("function", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")

        return features

    def _measure_morphology_improvements(self, request: MorphologyAnalysisRequest, analysis: Dict[str, Any], adaptations: Dict[str, Any]) -> Dict[str, float]:
        """Ù‚ÙŠØ§Ø³ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ø¯Ø§Ø¡ Ø§Ù„ØµØ±Ù"""

        improvements = {}

        # ØªØ­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØµØ±Ù
        avg_morphology_accuracy = np.mean([adapt.get("morphology_accuracy", 0.6) for adapt in adaptations.values()])
        baseline_morphology_accuracy = 0.5
        morphology_accuracy_improvement = ((avg_morphology_accuracy - baseline_morphology_accuracy) / baseline_morphology_accuracy) * 100
        improvements["morphology_accuracy_improvement"] = max(0, morphology_accuracy_improvement)

        # ØªØ­Ø³Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        avg_root_extraction = np.mean([adapt.get("root_extraction_accuracy", 0.7) for adapt in adaptations.values()])
        baseline_root_extraction = 0.6
        root_extraction_improvement = ((avg_root_extraction - baseline_root_extraction) / baseline_root_extraction) * 100
        improvements["root_extraction_improvement"] = max(0, root_extraction_improvement)

        # ØªØ­Ø³Ù† ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        avg_pattern_recognition = np.mean([adapt.get("pattern_recognition", 0.65) for adapt in adaptations.values()])
        baseline_pattern_recognition = 0.55
        pattern_recognition_improvement = ((avg_pattern_recognition - baseline_pattern_recognition) / baseline_pattern_recognition) * 100
        improvements["pattern_recognition_improvement"] = max(0, pattern_recognition_improvement)

        # ØªØ­Ø³Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯
        avg_affix_analysis = np.mean([adapt.get("affix_analysis", 0.6) for adapt in adaptations.values()])
        baseline_affix_analysis = 0.5
        affix_analysis_improvement = ((avg_affix_analysis - baseline_affix_analysis) / baseline_affix_analysis) * 100
        improvements["affix_analysis_improvement"] = max(0, affix_analysis_improvement)

        # ØªØ­Ø³Ù† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø°Ø¹
        avg_stem_identification = np.mean([adapt.get("stem_identification", 0.55) for adapt in adaptations.values()])
        baseline_stem_identification = 0.45
        stem_identification_improvement = ((avg_stem_identification - baseline_stem_identification) / baseline_stem_identification) * 100
        improvements["stem_identification_improvement"] = max(0, stem_identification_improvement)

        # ØªØ­Ø³Ù† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„ØµØ±ÙÙŠ
        total_adaptations = sum(adapt.get("total_adaptations", 0) for adapt in adaptations.values())
        morphology_complexity_improvement = total_adaptations * 12  # ÙƒÙ„ ØªÙƒÙŠÙ ØµØ±ÙÙŠ = 12% ØªØ­Ø³Ù†
        improvements["morphology_complexity_improvement"] = morphology_complexity_improvement

        return improvements

    def _extract_morphology_learning_insights(self, request: MorphologyAnalysisRequest, analysis: Dict[str, Any], improvements: Dict[str, float]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØµØ±ÙÙŠ"""

        insights = []

        if improvements["morphology_accuracy_improvement"] > 20:
            insights.append("Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ± Ø­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø­ÙˆØ¸")

        if improvements["root_extraction_improvement"] > 15:
            insights.append("Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ù…Ù…ØªØ§Ø²Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

        if improvements["pattern_recognition_improvement"] > 18:
            insights.append("Ø§Ù„Ù†Ø¸Ø§Ù… Ù†Ø¬Ø­ ÙÙŠ ØªØ­Ø³ÙŠÙ† ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©")

        if improvements["affix_analysis_improvement"] > 20:
            insights.append("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø²ÙˆØ§Ø¦Ø¯ ØªØ­Ø³Ù† Ù…Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø®Ø¨ÙŠØ±")

        if improvements["stem_identification_improvement"] > 22:
            insights.append("ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø°Ø¹ Ø£ØµØ¨Ø­ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„ØªÙƒÙŠÙ")

        if improvements["morphology_complexity_improvement"] > 80:
            insights.append("Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ© Ø§Ù„Ù…ØªÙƒÙŠÙØ© ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„ØµØ±ÙÙŠ Ø¨ÙƒÙØ§Ø¡Ø©")

        # Ø±Ø¤Ù‰ Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø©
        if len(request.word) > 6:
            insights.append("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØªØ¹Ø§Ù…Ù„ Ø¨ÙƒÙØ§Ø¡Ø© Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")

        if request.context:
            insights.append("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ ÙŠØ­Ø³Ù† Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ")

        return insights

    def _generate_morphology_next_cycle_recommendations(self, improvements: Dict[str, float], insights: List[str]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØµØ±ÙÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©"""

        recommendations = []

        avg_improvement = np.mean(list(improvements.values()))

        if avg_improvement > 30:
            recommendations.append("Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠØ©")
            recommendations.append("ØªØ¬Ø±Ø¨Ø© ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ Ø£Ø¹Ù…Ù‚ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")
        elif avg_improvement > 15:
            recommendations.append("Ø²ÙŠØ§Ø¯Ø© Ù‚ÙˆØ© Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„ØµØ±ÙÙŠ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹")
            recommendations.append("Ø¥Ø¶Ø§ÙØ© Ù‚ÙˆØ§Ø¹Ø¯ ØµØ±ÙÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©")
        else:
            recommendations.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„ØµØ±ÙÙŠ")
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„ØµØ±Ù")

        # ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø©
        if "Ø§Ù„Ø¬Ø°ÙˆØ±" in str(insights):
            recommendations.append("Ø§Ù„ØªÙˆØ³Ø¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

        if "Ø§Ù„Ø£ÙˆØ²Ø§Ù†" in str(insights):
            recommendations.append("ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ…ÙŠÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù†")

        if "Ù…Ø¹Ù‚Ø¯Ø©" in str(insights):
            recommendations.append("ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")

        return recommendations

    def _save_morphology_learning(self, request: MorphologyAnalysisRequest, result: MorphologyAnalysisResult):
        """Ø­ÙØ¸ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØµØ±ÙÙŠ"""

        learning_entry = {
            "timestamp": datetime.now().isoformat(),
            "word": request.word,
            "context": request.context,
            "analysis_depth": request.analysis_depth,
            "success": result.success,
            "root": result.root,
            "pattern": result.pattern,
            "performance_improvements": result.performance_improvements,
            "learning_insights": result.learning_insights
        }

        word_key = f"{request.word}_{request.analysis_depth}"
        if word_key not in self.morphology_learning_database:
            self.morphology_learning_database[word_key] = []

        self.morphology_learning_database[word_key].append(learning_entry)

        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 3 Ø¥Ø¯Ø®Ø§Ù„Ø§Øª ÙÙ‚Ø·
        if len(self.morphology_learning_database[word_key]) > 3:
            self.morphology_learning_database[word_key] = self.morphology_learning_database[word_key][-3:]

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø®Ø¨ÙŠØ±...")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„ØµØ±ÙÙŠ
    morphology_analyzer = ExpertGuidedArabicMorphologyAnalyzer()

    # ÙƒÙ„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø±Ø¨ÙŠØ©
    test_words = [
        "Ø§Ù„ÙƒØ§ØªØ¨",
        "ÙˆÙ…ÙƒØªÙˆØ¨Ø©",
        "ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ†",
        "Ø§Ø³ØªÙƒØªØ§Ø¨Ù‡Ù…",
        "ÙØ§Ù„Ù…Ø¯Ø±Ø³Ø©"
    ]

    for word in test_words:
        print(f"\n{'='*50}")
        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©: {word}")

        # Ø·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ
        morphology_request = MorphologyAnalysisRequest(
            word=word,
            context="Ø¬Ù…Ù„Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚",
            analysis_depth="comprehensive",
            morphology_aspects=["root", "pattern", "affixes", "stem"],
            expert_guidance_level="adaptive",
            learning_enabled=True
        )

        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ
        morphology_result = morphology_analyzer.analyze_morphology_with_expert_guidance(morphology_request)

        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØµØ±ÙÙŠØ©
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ:")
        print(f"   âœ… Ø§Ù„Ù†Ø¬Ø§Ø­: {morphology_result.success}")
        print(f"   ğŸŒ± Ø§Ù„Ø¬Ø°Ø±: {morphology_result.root}")
        print(f"   âš–ï¸ Ø§Ù„ÙˆØ²Ù†: {morphology_result.pattern}")
        print(f"   ğŸŒ¿ Ø§Ù„Ø¬Ø°Ø¹: {morphology_result.stem}")
        print(f"   â¬…ï¸ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª: {morphology_result.prefixes}")
        print(f"   â¡ï¸ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚: {morphology_result.suffixes}")

        if morphology_result.confidence_scores:
            print(f"   ğŸ“ˆ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø«Ù‚Ø©:")
            for aspect, score in morphology_result.confidence_scores.items():
                print(f"      {aspect}: {score:.2%}")

        if morphology_result.performance_improvements:
            print(f"   ğŸ“ˆ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡:")
            for metric, improvement in morphology_result.performance_improvements.items():
                print(f"      {metric}: {improvement:.1f}%")

        if morphology_result.learning_insights:
            print(f"   ğŸ§  Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù…:")
            for insight in morphology_result.learning_insights:
                print(f"      â€¢ {insight}")

if __name__ == "__main__":
    main()
